# ğŸ”§ LLM Tool Calling with TanStack Store & W&B Inference

> A complete guide to building apps where an LLM calls functions,
> using **TanStack Store** for state management, **SSE streaming**
> for real-time responses, and **W&B Inference API** as the LLM backend.
>
> Framework-agnostic. Works with Preact, React, Vue, Svelte, or vanilla JS.

---

## Table of Contents

1.  [Architecture Overview](#1-architecture-overview)
2.  [TanStack Store Crash Course](#2-tanstack-store-crash-course)
3.  [Chat State Design](#3-chat-state-design)
4.  [Settings Store (Persisted)](#4-settings-store-persisted)
5.  [Defining Tools](#5-defining-tools)
6.  [The System Prompt](#6-the-system-prompt)
7.  [Sending the Request](#7-sending-the-request)
8.  [Parsing the SSE Stream](#8-parsing-the-sse-stream)
9.  [Executing Tools](#9-executing-tools)
10. [The Follow-Up Loop](#10-the-follow-up-loop)
11. [Message Serialization](#11-message-serialization)
12. [Server Proxy (Bun)](#12-server-proxy-bun)
13. [W&B Inference API Reference](#13-wb-inference-api-reference)
14. [UI Integration](#14-ui-integration)
15. [Adapting for Different Apps](#15-adapting-for-different-apps)
16. [Gotchas & Debugging](#16-gotchas--debugging)
17. [Quick Reference Card](#17-quick-reference-card)

---

## 1. Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          TOOL CALLING LIFECYCLE                            â”‚
â”‚                                                                           â”‚
â”‚                                                                           â”‚
â”‚   User types:  "check the weather and set AC to 72"                       â”‚
â”‚       â”‚                                                                   â”‚
â”‚       â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚ TanStack Store â”‚  chatStore.setState(s => ({...s,                     â”‚
â”‚   â”‚ (add user msg) â”‚    messages: [...s.messages, userMsg]                 â”‚
â”‚   â”‚               â”‚  }))                                                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚           â”‚                                                               â”‚
â”‚           â–¼                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  POST /api/chat   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚   â”‚   Frontend     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Bun Server   â”‚                   â”‚
â”‚   â”‚   (browser)    â”‚  {messages,       â”‚  (proxy)      â”‚                   â”‚
â”‚   â”‚                â”‚   tools,          â”‚               â”‚                   â”‚
â”‚   â”‚                â”‚   stream:true}    â”‚  hides API    â”‚                   â”‚
â”‚   â”‚                â”‚                   â”‚  key from     â”‚                   â”‚
â”‚   â”‚                â”‚                   â”‚  browser      â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                                   â”‚                           â”‚
â”‚           â”‚                                   â”‚  POST /v1/chat/completionsâ”‚
â”‚           â”‚                                   â–¼                           â”‚
â”‚           â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚           â”‚                           â”‚  W&B Inferenceâ”‚                   â”‚
â”‚           â”‚                           â”‚  API          â”‚                   â”‚
â”‚           â”‚                           â”‚              â”‚                   â”‚
â”‚           â”‚                           â”‚  Kimi-K2.5   â”‚                   â”‚
â”‚           â”‚                           â”‚  Qwen        â”‚                   â”‚
â”‚           â”‚                           â”‚  Llama       â”‚                   â”‚
â”‚           â”‚                           â”‚  etc.        â”‚                   â”‚
â”‚           â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚           â”‚                                   â”‚                           â”‚
â”‚           â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€ SSE stream â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚           â”‚                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚ streamResponse â”‚  Parses 3 delta types:                               â”‚
â”‚   â”‚ ()             â”‚    â€¢ reasoning_content  (thinking)                   â”‚
â”‚   â”‚                â”‚    â€¢ tool_calls         (function calls)             â”‚
â”‚   â”‚                â”‚    â€¢ content            (text response)              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚           â”‚                                                               â”‚
â”‚           â”‚  tool_calls found?                                            â”‚
â”‚           â”‚                                                               â”‚
â”‚     NO    â”‚    YES                                                        â”‚
â”‚     â”‚     â”‚     â”‚                                                         â”‚
â”‚     â”‚     â”‚     â–¼                                                         â”‚
â”‚     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚     â”‚     â”‚  â”‚ executeTool()     â”‚  Run each tool locally                 â”‚
â”‚     â”‚     â”‚  â”‚                  â”‚  (API calls, DOM updates,              â”‚
â”‚     â”‚     â”‚  â”‚                  â”‚   postMessage, etc.)                    â”‚
â”‚     â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚     â”‚     â”‚           â”‚                                                   â”‚
â”‚     â”‚     â”‚           â–¼                                                   â”‚
â”‚     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚     â”‚     â”‚  â”‚ Add role:"tool"   â”‚  Store results in chatStore            â”‚
â”‚     â”‚     â”‚  â”‚ messages          â”‚                                         â”‚
â”‚     â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚     â”‚     â”‚           â”‚                                                   â”‚
â”‚     â”‚     â”‚           â–¼                                                   â”‚
â”‚     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚     â”‚     â”‚  â”‚ sendFollowUp()    â”‚  POST /api/chat again â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚     â”‚     â”‚  â”‚ (recursive)       â”‚                              â”‚          â”‚
â”‚     â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              loops back to   â”‚          â”‚
â”‚     â”‚     â”‚                                   streamResponse() â”‚          â”‚
â”‚     â–¼     â”‚                                                    â”‚          â”‚
â”‚   DONE    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚   (show                                                                   â”‚
â”‚    text)   The loop repeats until LLM returns text with NO tool_calls.   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The protocol in one sentence**: Send tools + messages â†’ LLM replies with
`tool_calls` â†’ you execute them â†’ send results back â†’ LLM replies with text
(or more tool_calls).

This is the **OpenAI tool calling standard**. Kimi-K2.5, GPT-4, Claude,
Mistral, Qwen, Llama, and all models on W&B Inference support it.

---

## 2. TanStack Store Crash Course

TanStack Store is a **tiny** (~1KB) reactive state manager. No reducers,
no actions, no boilerplate. Just stores you read and write.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TANSTACK STORE API                           â”‚
â”‚                                                              â”‚
â”‚  import { Store, batch } from '@tanstack/store'              â”‚
â”‚  import { useStore } from '@tanstack/preact-store'           â”‚
â”‚  // or: '@tanstack/react-store', '@tanstack/vue-store', etc  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  CREATE                                             â”‚     â”‚
â”‚  â”‚  const store = new Store<Type>({ initial: "value" })â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  UPDATE                                             â”‚     â”‚
â”‚  â”‚  store.setState(s => ({ ...s, field: newVal }))     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  READ (in components â€” auto-subscribes)             â”‚     â”‚
â”‚  â”‚  const val = useStore(store, s => s.field)          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  READ (outside components)                          â”‚     â”‚
â”‚  â”‚  const val = store.state.field                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  BATCH (multiple updates â†’ single re-render)        â”‚     â”‚
â”‚  â”‚  batch(() => {                                      â”‚     â”‚
â”‚  â”‚    storeA.setState(...)                              â”‚     â”‚
â”‚  â”‚    storeB.setState(...)                              â”‚     â”‚
â”‚  â”‚  })                                                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  SUBSCRIBE (side effects outside components)        â”‚     â”‚
â”‚  â”‚  const { unsubscribe } = store.subscribe(() => {    â”‚     â”‚
â”‚  â”‚    console.log("changed:", store.state)              â”‚     â”‚
â”‚  â”‚  })                                                 â”‚     â”‚
â”‚  â”‚                                                     â”‚     â”‚
â”‚  â”‚  âš ï¸  Returns { unsubscribe }, NOT a bare function!  â”‚     â”‚
â”‚  â”‚  âœ… const { unsubscribe } = store.subscribe(cb)     â”‚     â”‚
â”‚  â”‚  âŒ const unsub = store.subscribe(cb); unsub()      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  âš ï¸  NEVER use useState, useReducer, or useEffect.          â”‚
â”‚      TanStack Store replaces all of them.                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Install

```bash
bun add @tanstack/store @tanstack/preact-store
# or: @tanstack/react-store, @tanstack/vue-store, @tanstack/svelte-store
```

---

## 3. Chat State Design

Two stores handle all chat state: one for the conversation, one for user settings.

### ChatMsg Type

```typescript
interface ChatMsg {
  role: "user" | "assistant" | "tool";
  content: string;
  id: string;                // unique per message (for keying UI lists)
  toolName?: string;         // which tool was called (on role:"tool")
  toolArgs?: any;            // parsed arguments (for UI display)
  toolCallId?: string;       // links to the assistant's tool_calls[].id
  toolCalls?: any[];         // the raw tool_calls array (on assistant msgs)
  imageUrl?: string;         // base64 data URL for vision models
}
```

### Chat Store

```typescript
import { Store } from '@tanstack/store'

interface ChatState {
  messages: ChatMsg[];
  loading: boolean;          // true while waiting for LLM
  toolPhase: string | null;  // "get_weather..." shown during execution
  streamingText: string;     // partial text while streaming
  reasoningText: string;     // model's thinking (Kimi/DeepSeek)
  inputText: string;         // bound to the input field
  thinkingExpanded: boolean; // toggle for thinking bubble UI
}

const chatStore = new Store<ChatState>({
  messages: [],
  loading: false,
  toolPhase: null,
  streamingText: "",
  reasoningText: "",
  inputText: "",
  thinkingExpanded: false,
});
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CHAT STATE FLOW                              â”‚
â”‚                                                              â”‚
â”‚  User sends message                                          â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€â–º messages: [..., {role:"user", content:"..."}]         â”‚
â”‚    â”œâ”€â–º loading: true                                         â”‚
â”‚    â”œâ”€â–º streamingText: ""                                     â”‚
â”‚    â”‚                                                         â”‚
â”‚  SSE reasoning chunks arrive                                 â”‚
â”‚    â”œâ”€â–º reasoningText: "Let me think..."  (shown in UI)       â”‚
â”‚    â”‚                                                         â”‚
â”‚  SSE tool_call chunks arrive                                 â”‚
â”‚    â”œâ”€â–º toolPhase: "get_weather..."       (shown as badge)    â”‚
â”‚    â”œâ”€â–º reasoningText: ""                 (cleared)           â”‚
â”‚    â”‚                                                         â”‚
â”‚  Stream ends, tools execute                                  â”‚
â”‚    â”œâ”€â–º messages: [..., {role:"assistant", toolCalls:[...]}]   â”‚
â”‚    â”œâ”€â–º messages: [..., {role:"tool", content:"{...}"}]       â”‚
â”‚    â”œâ”€â–º sendFollowUp()                                        â”‚
â”‚    â”‚                                                         â”‚
â”‚  SSE content chunks arrive                                   â”‚
â”‚    â”œâ”€â–º streamingText: "The weather is..."                    â”‚
â”‚    â”œâ”€â–º messages[-1].content updated live                     â”‚
â”‚    â”‚                                                         â”‚
â”‚  Stream ends, no tools                                       â”‚
â”‚    â”œâ”€â–º loading: false                                        â”‚
â”‚    â””â”€â–º streamingText: ""                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Settings Store (Persisted)

User preferences persisted to localStorage. Controls model, temperature,
which tools are enabled, etc.

```typescript
interface SettingsState {
  llmModel: string;           // "moonshotai/Kimi-K2.5"
  apiKey: string;             // user's own W&B key (optional)
  projectId: string;          // W&B project header
  temperature: number;        // 0.0 â€“ 2.0
  topP: number;               // 0.0 â€“ 1.0
  topK: number;               // 1 â€“ 100
  repetitionPenalty: number;   // 1.0 â€“ 2.0
  maxTokens: number;          // 1000 â€“ 30000
  systemPrompt: string;       // custom override
  disabledTools: string[];    // ["search_web"] â€” tools user turned off
}

const STORAGE_KEY = "app_settings";

const DEFAULT_SETTINGS: SettingsState = {
  llmModel: "moonshotai/Kimi-K2.5",
  apiKey: "",
  projectId: "",
  temperature: 0.55,
  topP: 0.9,
  topK: 40,
  repetitionPenalty: 1.05,
  maxTokens: 16384,
  systemPrompt: "",
  disabledTools: [],
};

function loadSettings(): SettingsState {
  try {
    const raw = localStorage.getItem(STORAGE_KEY);
    if (raw) return { ...DEFAULT_SETTINGS, ...JSON.parse(raw) };
  } catch {}
  return { ...DEFAULT_SETTINGS };
}

function saveSettings(s: SettingsState) {
  try { localStorage.setItem(STORAGE_KEY, JSON.stringify(s)); }
  catch {}
}

const settingsStore = new Store<SettingsState>(loadSettings());

// Auto-save on every change (subscribe returns {unsubscribe})
settingsStore.subscribe(() => saveSettings(settingsStore.state));
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SETTINGS PERSISTENCE PATTERN                    â”‚
â”‚                                                              â”‚
â”‚  App starts                                                  â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€â–º loadSettings() reads localStorage                     â”‚
â”‚    â”œâ”€â–º Merges with DEFAULT_SETTINGS (handles new fields)     â”‚
â”‚    â”œâ”€â–º Creates Store with loaded state                       â”‚
â”‚    â”‚                                                         â”‚
â”‚  User changes a setting                                      â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€â–º settingsStore.setState(s => ({...s, temp: 0.7}))      â”‚
â”‚    â”œâ”€â–º subscribe() fires â†’ saveSettings() writes localStorageâ”‚
â”‚    â”‚                                                         â”‚
â”‚  App restarts                                                â”‚
â”‚    â””â”€â–º loadSettings() restores everything                    â”‚
â”‚                                                              â”‚
â”‚  âš ï¸  Always merge with defaults: { ...DEFAULT, ...saved }   â”‚
â”‚      This prevents crashes when you add new settings later.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Defining Tools

Tools are JSON Schema objects describing functions the LLM can call.

### The Format

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TOOL DEFINITION SHAPE                        â”‚
â”‚                                                              â”‚
â”‚  {                                                           â”‚
â”‚    type: "function",          â—„â”€â”€ always "function"          â”‚
â”‚    function: {                                               â”‚
â”‚      name: "snake_case_name", â—„â”€â”€ what the LLM calls        â”‚
â”‚      description: "...",      â—„â”€â”€ the LLM READS this!       â”‚
â”‚      parameters: {            â—„â”€â”€ JSON Schema for args      â”‚
â”‚        type: "object",                                       â”‚
â”‚        properties: {                                         â”‚
â”‚          arg1: { type: "string", description: "..." },       â”‚
â”‚          arg2: { type: "number" }                            â”‚
â”‚        },                                                    â”‚
â”‚        required: ["arg1"]                                    â”‚
â”‚      }                                                       â”‚
â”‚    }                                                         â”‚
â”‚  }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example: Smart Home App

```typescript
const TOOL_DEFS = [
  {
    type: "function",
    function: {
      name: "get_weather",
      description:
        "Get current weather for a city. Returns temperature, " +
        "humidity, conditions. Call this before making thermostat " +
        "decisions.",
      parameters: {
        type: "object",
        properties: {
          city: {
            type: "string",
            description: "City name (e.g. 'Tokyo', 'New York')"
          }
        },
        required: ["city"]
      },
    },
  },
  {
    type: "function",
    function: {
      name: "set_thermostat",
      description:
        "Set home thermostat. temp in Â°F (60-85). " +
        "mode: 'heat', 'cool', or 'auto'.",
      parameters: {
        type: "object",
        properties: {
          temp: { type: "number", description: "Temperature in Â°F" },
          mode: {
            type: "string",
            enum: ["heat", "cool", "auto"],
            description: "HVAC mode"
          },
        },
        required: ["temp"],
      },
    },
  },
  {
    type: "function",
    function: {
      name: "control_lights",
      description:
        "Control smart lights. room: which room. " +
        "brightness: 0=off, 100=full. color: hex color code.",
      parameters: {
        type: "object",
        properties: {
          room:       { type: "string", description: "Room name" },
          brightness: { type: "number", description: "0-100" },
          color:      { type: "string", description: "Hex (#ff0000)" },
        },
        required: ["room", "brightness"],
      },
    },
  },
  {
    type: "function",
    function: {
      name: "search_web",
      description: "Search the web for current information.",
      parameters: {
        type: "object",
        properties: {
          query: { type: "string" },
          count: { type: "number", description: "Results (1-10)" },
        },
        required: ["query"],
      },
    },
  },
  {
    type: "function",
    function: {
      name: "get_app_state",
      description:
        "Get current state of all devices â€” thermostat, lights, " +
        "locks. Call this to check before making changes.",
      parameters: { type: "object", properties: {} },  // no params
    },
  },
];
```

### Description Writing Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 WRITING GOOD DESCRIPTIONS                     â”‚
â”‚                                                               â”‚
â”‚  The LLM only "sees" the name, description, and schema.      â”‚
â”‚  Your description IS your documentation. Make it count.       â”‚
â”‚                                                               â”‚
â”‚  âœ… DO                               âŒ DON'T                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  List valid enum values              Vague one-liners         â”‚
â”‚  "mode: 'heat','cool','auto'"        "Controls the AC"       â”‚
â”‚                                                               â”‚
â”‚  Say WHEN to call it                 Assume LLM knows         â”‚
â”‚  "Call before making                 your app's logic         â”‚
â”‚   thermostat decisions"                                       â”‚
â”‚                                                               â”‚
â”‚  Give ranges & units                 Leave bounds open        â”‚
â”‚  "temp in Â°F (60-85)"               "Set temperature"        â”‚
â”‚                                                               â”‚
â”‚  Explain the return value            Only describe input      â”‚
â”‚  "Returns temperature,                                        â”‚
â”‚   humidity, conditions"                                       â”‚
â”‚                                                               â”‚
â”‚  Use snake_case for names            camelCase or spaces      â”‚
â”‚  set_thermostat âœ…                   setThermostat âŒ         â”‚
â”‚                                                               â”‚
â”‚  Mark truly required params          Make everything optional â”‚
â”‚  required: ["city"] âœ…               required: [] âŒ          â”‚
â”‚                                                               â”‚
â”‚  Put ordering hints                  Hope for the best        â”‚
â”‚  "Call FIRST" / "Call AFTER x"       (LLM guesses order)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. The System Prompt

The system prompt tells the LLM **who it is** and **how to use tools**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SYSTEM PROMPT STRUCTURE                           â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  1. PERSONA            â”‚  Who is the AI?                   â”‚
â”‚  â”‚  "You are a smart home â”‚  Sets tone, expertise,            â”‚
â”‚  â”‚   assistant..."        â”‚  personality.                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚             â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  2. TOOL SUMMARY       â”‚  List each tool with usage hints  â”‚
â”‚  â”‚  "Use aggressively"    â”‚  Tell it WHEN to call each one    â”‚
â”‚  â”‚  "Call FIRST"          â”‚  Ordering hints matter!           â”‚
â”‚  â”‚  "Call MULTIPLE times" â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚             â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  3. WORKFLOWS          â”‚  Multi-step recipes               â”‚
â”‚  â”‚  "First check state,   â”‚  Show the LLM how to chain tools â”‚
â”‚  â”‚   then adjust,         â”‚  for complex tasks.               â”‚
â”‚  â”‚   then confirm"        â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚             â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  4. CONSTRAINTS        â”‚  Hard rules & failure handling    â”‚
â”‚  â”‚  "If tool fails,       â”‚  Prevents hallucinating results  â”‚
â”‚  â”‚   say so"              â”‚  the tool didn't return.         â”‚
â”‚  â”‚  "NEVER guess state"   â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example

```typescript
const SYSTEM_PROMPT = `You are a helpful smart home assistant.
You control lights, thermostat, and can search the web.

## YOUR TOOLS:
- **get_weather**: Check weather. Call FIRST before adjusting thermostat.
- **set_thermostat**: Set temperature and HVAC mode.
- **control_lights**: Set room brightness and color.
- **search_web**: Look up information online.
- **get_app_state**: Check current device states. Call this before
  describing what's currently set.

## WORKFLOWS:
1. "Make it warmer" â†’ get_app_state â†’ set_thermostat
2. "Movie mode" â†’ control_lights(living_room, 20) + control_lights(kitchen, 0)
3. "Going to bed" â†’ control_lights(all rooms, 0) + set_thermostat(68, heat)

## RULES:
- ALWAYS call get_app_state before saying what's currently set
- If a tool call FAILS, tell the user â€” don't pretend it worked
- For multi-device requests, call ALL tools (don't ask for confirmation)
- Keep responses concise â€” the app shows device states visually`;
```

---

## 7. Sending the Request

### Request Shape

```
POST /api/chat
Content-Type: application/json

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                                            â”‚
â”‚   "messages": [                                              â”‚
â”‚     { "role": "system", "content": "You are..." },          â”‚
â”‚     { "role": "user",   "content": "make it warmer" }       â”‚
â”‚   ],                                                         â”‚
â”‚                                                              â”‚
â”‚   "tools": [ ...TOOL_DEFS ],      â—„â”€â”€ array of tool schemas â”‚
â”‚   "tool_choice": "auto",          â—„â”€â”€ see options below     â”‚
â”‚   "stream": true,                  â—„â”€â”€ SSE streaming         â”‚
â”‚                                                              â”‚
â”‚   "max_tokens": 16384,                                       â”‚
â”‚   "temperature": 0.55,                                       â”‚
â”‚   "top_p": 0.9,                                              â”‚
â”‚   "top_k": 40,                                               â”‚
â”‚   "repetition_penalty": 1.05                                 â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### `tool_choice` Values

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "auto"       LLM decides whether to call tools or not      â”‚
â”‚               â†’ Use 99% of the time                         â”‚
â”‚                                                              â”‚
â”‚  "none"       LLM will NOT call tools (text only)           â”‚
â”‚               â†’ Useful for forcing text after tool results   â”‚
â”‚                                                              â”‚
â”‚  "required"   LLM MUST call at least one tool               â”‚
â”‚               â†’ Use for action-only flows                    â”‚
â”‚                                                              â”‚
â”‚  { "type":    Force a SPECIFIC tool                         â”‚
â”‚    "function",â†’ Use for guided step-by-step workflows        â”‚
â”‚    "function":                                               â”‚
â”‚    {"name":"get_weather"} }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The `sendMessage()` Function

```typescript
async function sendMessage(userInput: string) {
  const input = userInput.trim();
  if (!input) return;

  // 1 â”€â”€ Add user message to store
  const userMsg: ChatMsg = {
    role: "user",
    content: input,
    id: `u_${Date.now()}`
  };
  chatStore.setState(s => ({
    ...s,
    messages: [...s.messages, userMsg],
    loading: true,
    streamingText: "",
    toolPhase: null,
  }));

  try {
    // 2 â”€â”€ Serialize conversation history
    const msgs = serializeMsgs(chatStore.state.messages);

    // 3 â”€â”€ Filter out disabled tools
    const cfg = settingsStore.state;
    const activeTools = TOOL_DEFS.filter(
      t => !cfg.disabledTools.includes(t.function.name)
    );

    // 4 â”€â”€ POST to server proxy
    const resp = await fetch("/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        messages: [
          { role: "system", content: cfg.systemPrompt || SYSTEM_PROMPT },
          ...msgs,
        ],
        tools: activeTools.length ? activeTools : undefined,
        tool_choice: activeTools.length ? "auto" : undefined,
        stream: true,
        max_tokens: cfg.maxTokens,
        temperature: cfg.temperature,
        top_p: cfg.topP,
        top_k: cfg.topK,
        repetition_penalty: cfg.repetitionPenalty,
        ...(cfg.llmModel ? { model: cfg.llmModel } : {}),
        ...(cfg.apiKey ? { apiKey: cfg.apiKey } : {}),
      }),
    });

    if (!resp.ok) {
      const err = await resp.json().catch(() => ({ error: "Unknown" }));
      chatStore.setState(s => ({
        ...s,
        messages: [...s.messages, {
          role: "assistant",
          content: `Error: ${err.error || resp.status}`,
          id: `e_${Date.now()}`
        }],
        loading: false,
      }));
      return;
    }

    // 5 â”€â”€ Parse the SSE stream
    await streamResponse(resp);
  } catch (e: any) {
    chatStore.setState(s => ({
      ...s,
      messages: [...s.messages, {
        role: "assistant",
        content: `Network error: ${e.message}`,
        id: `ne_${Date.now()}`
      }],
      loading: false,
    }));
  }
}
```

---

## 8. Parsing the SSE Stream

The LLM sends **Server-Sent Events**. Tool call arguments arrive **in
fragments** that you must concatenate.

### Raw SSE From W&B / Kimi-K2.5

```
â”€â”€ Phase 1: Thinking (reasoning_content) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"choices":[{"delta":{"reasoning_content":"The user wants"}}]}
data: {"choices":[{"delta":{"reasoning_content":" it warmer. I should"}}]}
data: {"choices":[{"delta":{"reasoning_content":" check current state first."}}]}

â”€â”€ Phase 2: Tool calls (streamed in FRAGMENTS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"choices":[{"delta":{"tool_calls":[{
  "index": 0,
  "id": "call_abc123",
  "function": {"name": "get_app_state", "arguments": ""}
}]}}]}

data: {"choices":[{"delta":{"tool_calls":[{
  "index": 0,
  "function": {"arguments": "{}"}
}]}}]}

â”€â”€ (after follow-up with results, more tool calls) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"choices":[{"delta":{"tool_calls":[{
  "index": 0,
  "id": "call_def456",
  "function": {"name": "set_thermostat", "arguments": ""}
}]}}]}

data: {"choices":[{"delta":{"tool_calls":[{
  "index": 0,
  "function": {"arguments": "{\"te"}
}]}}]}

data: {"choices":[{"delta":{"tool_calls":[{
  "index": 0,
  "function": {"arguments": "mp\":74,\"mo"}
}]}}]}

data: {"choices":[{"delta":{"tool_calls":[{
  "index": 0,
  "function": {"arguments": "de\":\"heat\"}"}
}]}}]}

data: [DONE]
```

### Why Arguments Are Fragmented

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ARGUMENT STREAMING â€” THE KEY INSIGHT                 â”‚
â”‚                                                              â”‚
â”‚  The LLM generates tokens one at a time, including the       â”‚
â”‚  JSON arguments string. You get partial JSON:                â”‚
â”‚                                                              â”‚
â”‚  Chunk 1:  arguments: "{\"te"                                â”‚
â”‚  Chunk 2:  arguments: "mp\":74,\"mo"                         â”‚
â”‚  Chunk 3:  arguments: "de\":\"heat\"}"                       â”‚
â”‚                     â†“ concatenate                            â”‚
â”‚  Full:     arguments: "{\"temp\":74,\"mode\":\"heat\"}"      â”‚
â”‚                     â†“ JSON.parse()                           â”‚
â”‚  Result:   { temp: 74, mode: "heat" }                        â”‚
â”‚                                                              â”‚
â”‚  âš ï¸  NEVER JSON.parse() until the stream is fully DONE!     â”‚
â”‚  âš ï¸  Multiple tool calls use "index" to disambiguate.        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The `streamResponse()` Function

```typescript
async function streamResponse(resp: Response) {
  const reader = resp.body?.getReader();
  const decoder = new TextDecoder();
  let buffer = "";
  let accText = "";
  let accReasoning = "";
  const toolCalls: any[] = [];
  const assistantMsgId = `a_${Date.now()}`;

  // Add placeholder assistant message to UI
  chatStore.setState(s => ({
    ...s,
    messages: [...s.messages, {
      role: "assistant", content: "", id: assistantMsgId
    }],
    reasoningText: "",
  }));

  while (true) {
    const { done, value } = await reader!.read();
    if (done) break;
    buffer += decoder.decode(value, { stream: true });

    // SSE lines are separated by \n
    const lines = buffer.split("\n");
    buffer = lines.pop() || "";  // keep incomplete line in buffer

    for (const line of lines) {
      if (!line.startsWith("data: ")) continue;
      const data = line.slice(6).trim();
      if (data === "[DONE]") break;

      let chunk: any;
      try { chunk = JSON.parse(data); }
      catch { continue; }

      const delta = chunk.choices?.[0]?.delta;
      if (!delta) continue;

      // â”€â”€ Reasoning (Kimi-K2.5 / DeepSeek thinking) â”€â”€â”€â”€â”€â”€
      if (delta.reasoning_content) {
        accReasoning += delta.reasoning_content;
        chatStore.setState(s => ({
          ...s,
          reasoningText: accReasoning,
        }));
      }

      // â”€â”€ Text content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (delta.content) {
        accText += delta.content;
        chatStore.setState(s => ({
          ...s,
          streamingText: accText,
          reasoningText: "",  // clear thinking once text starts
          messages: s.messages.map(m =>
            m.id === assistantMsgId
              ? { ...m, content: accText }
              : m
          ),
        }));
      }

      // â”€â”€ Tool calls (accumulate by index!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (delta.tool_calls) {
        for (const tc of delta.tool_calls) {
          const idx = tc.index ?? 0;

          // First chunk for this tool â€” create entry
          if (!toolCalls[idx]) {
            toolCalls[idx] = {
              id: tc.id || "",
              type: "function",
              function: { name: "", arguments: "" },
            };
          }

          // Set name (arrives in first chunk only)
          if (tc.function?.name) {
            toolCalls[idx].function.name = tc.function.name;
          }

          // CONCATENATE arguments (arrives across many chunks)
          if (tc.function?.arguments) {
            toolCalls[idx].function.arguments +=
              tc.function.arguments;
          }

          // Update UI with current tool phase
          chatStore.setState(s => ({
            ...s,
            toolPhase: `${toolCalls[idx].function.name}...`,
            reasoningText: "",
          }));
        }
      }
    }
  }

  // â”€â”€ Stream finished â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  chatStore.setState(s => ({
    ...s,
    streamingText: "",
    reasoningText: "",
    toolPhase: null,
  }));

  // â”€â”€ Execute tools if any â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (toolCalls.length > 0) {
    // Attach tool_calls to the assistant message
    chatStore.setState(s => ({
      ...s,
      messages: s.messages.map(m =>
        m.id === assistantMsgId
          ? { ...m, toolCalls }
          : m
      ),
    }));

    // Execute each tool and collect results
    const toolResults: ChatMsg[] = [];
    for (const tc of toolCalls) {
      if (!tc.function.name) continue;
      chatStore.setState(s => ({
        ...s,
        toolPhase: tc.function.name,
      }));

      const args = safeJson(tc.function.arguments, {});
      const result = await executeTool(tc.function.name, args);

      toolResults.push({
        role: "tool",
        content: JSON.stringify(result),
        id: `t_${Date.now()}`,
        toolName: tc.function.name,
        toolArgs: args,
        toolCallId: tc.id,     // â—„â”€â”€ MUST match tc.id!
      });
    }

    // Add tool results to store
    chatStore.setState(s => ({
      ...s,
      messages: [...s.messages, ...toolResults],
      toolPhase: null,
    }));

    // Follow-up: send results back to LLM
    await sendFollowUp();
  } else {
    // No tools â€” just text. We're done.
    chatStore.setState(s => ({ ...s, loading: false }));
  }
}

// Safe JSON parse helper
function safeJson<T>(s: string, fallback: T): T {
  try { return JSON.parse(s); }
  catch { return fallback; }
}
```

### The Three Delta Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SSE DELTA TYPES                                 â”‚
â”‚                                                              â”‚
â”‚  delta.reasoning_content                                     â”‚
â”‚  â”œâ”€â”€ Only on thinking models (Kimi-K2.5, DeepSeek-R1)       â”‚
â”‚  â”œâ”€â”€ Arrives FIRST, before tool_calls or content             â”‚
â”‚  â”œâ”€â”€ Show as a collapsible "thinking" bubble in UI           â”‚
â”‚  â””â”€â”€ Clear it when content or tool_calls start               â”‚
â”‚                                                              â”‚
â”‚  delta.tool_calls                                            â”‚
â”‚  â”œâ”€â”€ Array of {index, id?, function: {name?, arguments?}}    â”‚
â”‚  â”œâ”€â”€ index disambiguates parallel tool calls (0, 1, 2...)    â”‚
â”‚  â”œâ”€â”€ id only appears in the FIRST chunk for each call        â”‚
â”‚  â”œâ”€â”€ name only appears in the FIRST chunk                    â”‚
â”‚  â”œâ”€â”€ arguments is FRAGMENTED â€” concatenate as strings!       â”‚
â”‚  â””â”€â”€ JSON.parse arguments ONLY after [DONE]                  â”‚
â”‚                                                              â”‚
â”‚  delta.content                                               â”‚
â”‚  â”œâ”€â”€ Normal text response â€” stream to UI character by char   â”‚
â”‚  â”œâ”€â”€ Mutually exclusive with tool_calls in same response     â”‚
â”‚  â””â”€â”€ May appear after tool results in follow-up              â”‚
â”‚                                                              â”‚
â”‚  âš ï¸  reasoning â†’ tool_calls OR content (never both at once) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Executing Tools

After streaming, you have `toolCalls[]` with `{name, arguments}`.
Parse and dispatch.

```typescript
async function executeTool(name: string, args: any): Promise<any> {
  switch (name) {

    case "get_weather": {
      try {
        const resp = await fetch(
          `/api/weather?city=${encodeURIComponent(args.city)}`
        );
        return await resp.json();
      } catch (e: any) {
        return { ok: false, reason: e.message };
      }
    }

    case "set_thermostat": {
      try {
        await fetch("/api/thermostat", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            temp: args.temp,
            mode: args.mode || "auto"
          }),
        });
        // Update app state so UI reflects change
        appStore.setState(s => ({
          ...s,
          thermostat: { temp: args.temp, mode: args.mode || "auto" }
        }));
        return { ok: true, temp: args.temp, mode: args.mode || "auto" };
      } catch (e: any) {
        return { ok: false, reason: e.message };
      }
    }

    case "control_lights": {
      // postMessage to a lights controller, or call an API
      await fetch("/api/lights", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(args),
      });
      return { ok: true, room: args.room, brightness: args.brightness };
    }

    case "search_web": {
      const resp = await fetch(
        `/api/search?q=${encodeURIComponent(args.query)}&n=${args.count || 5}`
      );
      const data = await resp.json();
      return { ok: true, results: data.results.slice(0, 5) };
    }

    case "get_app_state": {
      // Read directly from store â€” no network call needed
      return {
        ok: true,
        thermostat: appStore.state.thermostat,
        lights: appStore.state.lights,
        locks: appStore.state.locks,
      };
    }

    default:
      return { ok: false, reason: `Unknown tool: ${name}` };
  }
}
```

### Tool Result Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           WHAT executeTool() SHOULD RETURN                    â”‚
â”‚                                                              â”‚
â”‚  The return value is JSON.stringify'd and sent back to the   â”‚
â”‚  LLM as role:"tool" content. The LLM reads it to generate   â”‚
â”‚  its response.                                               â”‚
â”‚                                                              â”‚
â”‚  âœ… GOOD                                                     â”‚
â”‚  { ok: true, temp: 74, mode: "heat" }                        â”‚
â”‚  { ok: false, reason: "Device offline" }                     â”‚
â”‚  { ok: true, results: [{title:"...", url:"..."}] }           â”‚
â”‚                                                              â”‚
â”‚  âŒ BAD                                                      â”‚
â”‚  "success"               â† LLM can't reason about this      â”‚
â”‚  undefined               â† becomes "null", confusing         â”‚
â”‚  { data: <50KB blob> }   â† wastes context window             â”‚
â”‚  throw new Error(...)    â† breaks the tool loop entirely     â”‚
â”‚                                                              â”‚
â”‚  ğŸ“ RULES:                                                   â”‚
â”‚  â€¢ Always return { ok: true/false, ... }                     â”‚
â”‚  â€¢ On failure â†’ { ok: false, reason: "what went wrong" }     â”‚
â”‚  â€¢ On success â†’ include key data the LLM needs              â”‚
â”‚  â€¢ Keep it SMALL (<2KB) â€” this eats context window           â”‚
â”‚  â€¢ NEVER throw â€” always try/catch and return ok:false        â”‚
â”‚  â€¢ NEVER include raw HTML, base64, or giant arrays           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. The Follow-Up Loop

After tool execution, send results back. The LLM may call more tools
or finally produce text.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE RECURSIVE LOOP                         â”‚
â”‚                                                              â”‚
â”‚  sendMessage()                                               â”‚
â”‚      â”‚                                                       â”‚
â”‚      â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ streamResponseâ”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”‚  toolCalls.length > 0?                              â”‚
â”‚         â”‚                                                    â”‚
â”‚    NO â”€â”€â”¤â”€â”€ YES                                              â”‚
â”‚    â”‚    â”‚    â”‚                                                â”‚
â”‚    â”‚    â”‚    â–¼                                                â”‚
â”‚    â”‚    â”‚  execute tools                                      â”‚
â”‚    â”‚    â”‚  add role:"tool" msgs                               â”‚
â”‚    â”‚    â”‚    â”‚                                                â”‚
â”‚    â”‚    â”‚    â–¼                                                â”‚
â”‚    â”‚    â”‚  sendFollowUp() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚    â”‚    â”‚    â”‚                              â”‚                 â”‚
â”‚    â”‚    â”‚    â–¼                              â”‚                 â”‚
â”‚    â”‚    â”‚  streamResponse() â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚    â”‚    â”‚    â”‚                                                â”‚
â”‚    â”‚    â”‚    â””â”€â–º may loop again if LLM calls more tools       â”‚
â”‚    â”‚    â”‚                                                    â”‚
â”‚    â–¼    â”‚                                                    â”‚
â”‚  DONE   â”‚  Typical: 1-2 loops. Complex: 3-4 loops.           â”‚
â”‚  (text) â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
async function sendFollowUp() {
  try {
    const msgs = serializeMsgs(chatStore.state.messages);
    const cfg = settingsStore.state;
    const sysPrompt = cfg.systemPrompt || SYSTEM_PROMPT;
    const activeTools = TOOL_DEFS.filter(
      t => !cfg.disabledTools.includes(t.function.name)
    );

    // Use fewer tokens for follow-ups (LLM already has context)
    const followUpTokens = Math.max(
      1000,
      Math.round(cfg.maxTokens / 2)
    );

    const resp = await fetch("/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        messages: [
          { role: "system", content: sysPrompt },
          ...msgs,   // â—„â”€â”€ now includes tool result messages!
        ],
        tools: activeTools.length ? activeTools : undefined,
        tool_choice: activeTools.length ? "auto" : undefined,
        stream: true,
        max_tokens: followUpTokens,
        temperature: cfg.temperature,
        top_p: cfg.topP,
        top_k: cfg.topK,
        repetition_penalty: cfg.repetitionPenalty,
        ...(cfg.llmModel ? { model: cfg.llmModel } : {}),
        ...(cfg.apiKey ? { apiKey: cfg.apiKey } : {}),
      }),
    });

    if (resp.ok) {
      await streamResponse(resp);  // â—„â”€â”€ RECURSIVE!
    } else {
      chatStore.setState(s => ({ ...s, loading: false }));
    }
  } catch {
    chatStore.setState(s => ({ ...s, loading: false }));
  }
}
```

---

## 11. Message Serialization

The OpenAI API is **strict** about message format. Three types need special
shapes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THE THREE MESSAGE SHAPES                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€ Plain User â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  { role: "user", content: "make it warmer" }          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€ User with Image (Vision) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  { role: "user",                                      â”‚   â”‚
â”‚  â”‚    content: [                                         â”‚   â”‚
â”‚  â”‚      { type: "text", text: "what's this?" },          â”‚   â”‚
â”‚  â”‚      { type: "image_url",                             â”‚   â”‚
â”‚  â”‚        image_url: { url: "data:image/png;base64,..." }â”‚   â”‚
â”‚  â”‚      }                                                â”‚   â”‚
â”‚  â”‚    ]                                                  â”‚   â”‚
â”‚  â”‚  }                                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€ Assistant with Tool Calls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  { role: "assistant",                                 â”‚   â”‚
â”‚  â”‚    content: "",        â—„â”€â”€ often empty                â”‚   â”‚
â”‚  â”‚    tool_calls: [                                      â”‚   â”‚
â”‚  â”‚      { id: "call_abc123",                             â”‚   â”‚
â”‚  â”‚        type: "function",                              â”‚   â”‚
â”‚  â”‚        function: {                                    â”‚   â”‚
â”‚  â”‚          name: "set_thermostat",                      â”‚   â”‚
â”‚  â”‚          arguments: "{\"temp\":74}"                   â”‚   â”‚
â”‚  â”‚        }                                              â”‚   â”‚
â”‚  â”‚      }                                                â”‚   â”‚
â”‚  â”‚    ]                                                  â”‚   â”‚
â”‚  â”‚  }                                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€ Tool Result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  { role: "tool",                                      â”‚   â”‚
â”‚  â”‚    content: "{\"ok\":true,\"temp\":74}",              â”‚   â”‚
â”‚  â”‚    tool_call_id: "call_abc123",  â—„â”€â”€ MUST MATCH!      â”‚   â”‚
â”‚  â”‚    name: "set_thermostat"                             â”‚   â”‚
â”‚  â”‚  }                                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  âš ï¸  tool_call_id MUST match the id from tool_calls[]       â”‚
â”‚  âš ï¸  content in tool msgs MUST be a string (JSON.stringify)  â”‚
â”‚  âš ï¸  assistant msg MUST include tool_calls array             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Full Conversation Example

```
â”Œâ”€ What gets sent to the API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  [0] system:    "You are a smart home assistant..."          â”‚
â”‚  [1] user:      "make it warmer"                             â”‚
â”‚  [2] assistant: { content: "",                               â”‚
â”‚                   tool_calls: [{                             â”‚
â”‚                     id: "call_1",                            â”‚
â”‚                     function: {                              â”‚
â”‚                       name: "get_app_state",                 â”‚
â”‚                       arguments: "{}"                        â”‚
â”‚                     }                                        â”‚
â”‚                   }]                                         â”‚
â”‚                 }                                            â”‚
â”‚  [3] tool:      { content: "{\"thermostat\":{\"temp\":68}}", â”‚
â”‚                   tool_call_id: "call_1",                    â”‚
â”‚                   name: "get_app_state" }                    â”‚
â”‚  [4] assistant: { content: "",                               â”‚
â”‚                   tool_calls: [{                             â”‚
â”‚                     id: "call_2",                            â”‚
â”‚                     function: {                              â”‚
â”‚                       name: "set_thermostat",                â”‚
â”‚                       arguments: "{\"temp\":74}"             â”‚
â”‚                     }                                        â”‚
â”‚                   }]                                         â”‚
â”‚                 }                                            â”‚
â”‚  [5] tool:      { content: "{\"ok\":true,\"temp\":74}",      â”‚
â”‚                   tool_call_id: "call_2",                    â”‚
â”‚                   name: "set_thermostat" }                   â”‚
â”‚                                                              â”‚
â”‚  â†’ LLM responds: "Done! I've bumped the thermostat from     â”‚
â”‚    68Â°F to 74Â°F. It should warm up in a few minutes."        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The `serializeMsgs()` Function

```typescript
function serializeMsgs(messages: ChatMsg[]) {
  return messages.map(m => {

    // Assistant with tool calls â€” include tool_calls array
    if (m.role === "assistant" && m.toolCalls?.length) {
      return {
        role: "assistant",
        content: m.content || "",
        tool_calls: m.toolCalls,
      };
    }

    // Tool result â€” include tool_call_id and name
    if (m.role === "tool") {
      return {
        role: "tool",
        content: m.content,            // already JSON string
        tool_call_id: m.toolCallId,    // MUST match call id
        name: m.toolName,
      };
    }

    // User with image â€” multipart content array
    if (m.role === "user" && m.imageUrl) {
      return {
        role: "user",
        content: [
          ...(m.content
            ? [{ type: "text", text: m.content }]
            : []),
          {
            type: "image_url",
            image_url: { url: m.imageUrl },
          },
        ],
      };
    }

    // Default â€” plain text
    return { role: m.role, content: m.content };
  });
}
```

---

## 12. Server Proxy (Bun)

Your server sits between browser and W&B API. It hides API keys, rate-limits,
and pipes the SSE stream.

```typescript
// serve.ts â€” Bun HTTP server

const WANDB_API_KEY = process.env.WANDB_API_KEY || "";
const PORT = Number(process.env.PORT) || 3001;

// â”€â”€ Rate limiter (per IP, 20 requests/minute) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const rateLimits = new Map<string, number[]>();

function checkRateLimit(req: Request): boolean {
  const ip = req.headers.get("x-forwarded-for")
    || req.headers.get("cf-connecting-ip")
    || "unknown";
  const now = Date.now();
  const window = 60_000;  // 1 minute
  const max = 20;

  const times = (rateLimits.get(ip) || [])
    .filter(t => now - t < window);
  if (times.length >= max) return false;
  times.push(now);
  rateLimits.set(ip, times);
  return true;
}

// â”€â”€ Bun.serve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Bun.serve({
  port: PORT,
  async fetch(req) {
    const url = new URL(req.url);

    // â”€â”€ Chat API proxy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (url.pathname === "/api/chat" && req.method === "POST") {
      if (!checkRateLimit(req)) {
        return new Response(
          JSON.stringify({ error: "Rate limited" }),
          { status: 429 }
        );
      }

      let body: any;
      try { body = await req.json(); }
      catch {
        return new Response(
          JSON.stringify({ error: "Invalid JSON" }),
          { status: 400 }
        );
      }

      // Cap max tokens server-side
      const MAX_ALLOWED = 30000;
      const maxTokens = Math.min(
        Math.max(1, body.max_tokens || 20000),
        MAX_ALLOWED
      );
      const doStream = body.stream !== false;

      // Resolve API key (body override > env var)
      const apiKey = body.apiKey || WANDB_API_KEY;
      if (!apiKey) {
        return new Response(
          JSON.stringify({ error: "No API key configured" }),
          { status: 500 }
        );
      }

      // W&B project header (routes to model group)
      const projectHeader =
        body.project || "your-org/your-project";

      // â”€â”€ Forward to W&B Inference API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      let upstream: Response;
      try {
        upstream = await fetch(
          "https://api.inference.wandb.ai/v1/chat/completions",
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${apiKey}`,
              "OpenAI-Project": projectHeader,
            },
            body: JSON.stringify({
              model: body.model || "moonshotai/Kimi-K2.5",
              messages: body.messages,

              // Pass tools through to the LLM
              ...(body.tools?.length
                ? {
                    tools: body.tools,
                    tool_choice: body.tool_choice || "auto",
                  }
                : {}),

              stream: doStream,
              max_tokens: maxTokens,
              temperature:
                typeof body.temperature === "number"
                  ? body.temperature : 0.55,
              top_p:
                typeof body.top_p === "number"
                  ? body.top_p : 0.9,
              top_k:
                typeof body.top_k === "number"
                  ? body.top_k : 40,
              repetition_penalty:
                typeof body.repetition_penalty === "number"
                  ? body.repetition_penalty : 1.05,
            }),
            signal: AbortSignal.timeout(300_000), // 5 min
          }
        );
      } catch (e) {
        return new Response(
          JSON.stringify({ error: "Failed to connect to LLM" }),
          { status: 502 }
        );
      }

      if (!upstream.ok) {
        const errText = await upstream.text();
        return new Response(
          JSON.stringify({
            error: `Upstream ${upstream.status}: ${errText.slice(0, 200)}`
          }),
          { status: upstream.status }
        );
      }

      // â”€â”€ Streaming: pipe SSE as-is â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (doStream) {
        return new Response(upstream.body, {
          headers: {
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  // nginx: don't buffer
          },
        });
      }

      // â”€â”€ Non-streaming: forward JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      const data = await upstream.json();
      return new Response(JSON.stringify(data), {
        headers: { "Content-Type": "application/json" },
      });
    }

    // ... your other routes ...

    return new Response("Not found", { status: 404 });
  },
});

console.log(`Server running on :${PORT}`);
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHY A SERVER PROXY?                             â”‚
â”‚                                                              â”‚
â”‚  Browser â”€â”€â–º Your Server â”€â”€â–º W&B API                        â”‚
â”‚                                                              â”‚
â”‚  â€¢ API keys stay server-side (NEVER in browser JS)          â”‚
â”‚  â€¢ Rate limiting per IP                                      â”‚
â”‚  â€¢ max_tokens capped server-side (prevent abuse)             â”‚
â”‚  â€¢ Can swap models without changing frontend                 â”‚
â”‚  â€¢ SSE pass-through is zero-copy (pipe upstream.body)        â”‚
â”‚  â€¢ Add logging, monitoring, usage tracking                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 13. W&B Inference API Reference

Weights & Biases (W&B) hosts open-source models behind an OpenAI-compatible
API. You get access to Kimi-K2.5, Qwen, Llama, and others.

### Getting a W&B API Key

```
1. Go to https://wandb.ai/settings
2. Scroll to "API Keys"
3. Click "New Key" or copy existing
4. Set as env var: export WANDB_API_KEY="wandb_v1_..."
```

### API Endpoint

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  URL:     https://api.inference.wandb.ai/v1/chat/completionsâ”‚
â”‚  Method:  POST                                               â”‚
â”‚  Auth:    Bearer token (W&B API key)                        â”‚
â”‚  Format:  OpenAI-compatible (same as GPT-4 API)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Headers

```
Content-Type:   application/json
Authorization:  Bearer wandb_v1_...
OpenAI-Project: your-org/your-project    â† routes to model group
```

### Available Models (as of 2025)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model ID                     â”‚  Notes                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  moonshotai/Kimi-K2.5         â”‚  Thinking + tool calling     â”‚
â”‚  Qwen/Qwen3-235B-A22B        â”‚  Large MoE                   â”‚
â”‚  Qwen/Qwen2.5-Coder-32B      â”‚  Code-focused                â”‚
â”‚  meta-llama/Llama-3.3-70B     â”‚  General purpose             â”‚
â”‚  deepseek-ai/DeepSeek-R1      â”‚  Reasoning model             â”‚
â”‚  mistralai/Mistral-Large-2    â”‚  General purpose             â”‚
â”‚  google/gemma-2-27b           â”‚  Compact                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Check latest at: https://wandb.ai/site/inference
```

### Request Body

```typescript
{
  model: "moonshotai/Kimi-K2.5",
  messages: [
    { role: "system", content: "..." },
    { role: "user",   content: "..." },
  ],

  // Tool calling (optional)
  tools: TOOL_DEFS,           // array of tool definitions
  tool_choice: "auto",        // "auto" | "none" | "required"

  // Streaming
  stream: true,               // SSE mode

  // Generation parameters
  max_tokens: 16384,           // 1 â€“ 30000+
  temperature: 0.55,           // 0.0 (deterministic) â€“ 2.0 (creative)
  top_p: 0.9,                 // nucleus sampling
  top_k: 40,                  // top-k sampling
  repetition_penalty: 1.05,   // 1.0 = none, >1.0 = penalize repeats
}
```

### Kimi-K2.5 Special: `reasoning_content`

Kimi-K2.5 (and DeepSeek-R1) have a "thinking" phase. The model's internal
reasoning arrives as `delta.reasoning_content` before the actual response:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KIMI-K2.5 RESPONSE PHASES                                  â”‚
â”‚                                                              â”‚
â”‚  Phase 1: reasoning_content                                  â”‚
â”‚  â”œâ”€â”€ "The user wants to increase the temperature..."         â”‚
â”‚  â”œâ”€â”€ "Current thermostat is at 68Â°F..."                      â”‚
â”‚  â””â”€â”€ "I should call get_app_state first, then set_therm..." â”‚
â”‚                                                              â”‚
â”‚  Phase 2: tool_calls OR content                              â”‚
â”‚  â”œâ”€â”€ tool_calls: [{name: "get_app_state", args: "{}"}]      â”‚
â”‚  â””â”€â”€ (after follow-up) content: "I've set it to 74Â°F!"      â”‚
â”‚                                                              â”‚
â”‚  Show reasoning as a collapsible "thinking" bubble.          â”‚
â”‚  Clear it once real content starts streaming.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Recommended Parameters by Use Case

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Use Case           â”‚ Temp â”‚ TopP â”‚ TopK â”‚ RepP  â”‚ MaxTok   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tool-heavy agent   â”‚ 0.3  â”‚ 0.9  â”‚ 40   â”‚ 1.05  â”‚ 16384    â”‚
â”‚  Creative writing   â”‚ 0.9  â”‚ 0.95 â”‚ 80   â”‚ 1.1   â”‚ 8192     â”‚
â”‚  Code generation    â”‚ 0.2  â”‚ 0.85 â”‚ 30   â”‚ 1.0   â”‚ 16384    â”‚
â”‚  Chat assistant     â”‚ 0.55 â”‚ 0.9  â”‚ 40   â”‚ 1.05  â”‚ 16384    â”‚
â”‚  Deterministic Q&A  â”‚ 0.0  â”‚ 1.0  â”‚ 1    â”‚ 1.0   â”‚ 4096     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 14. UI Integration

### Reading Store in Components (Preact/React)

```typescript
import { useStore } from '@tanstack/preact-store'
// React: import { useStore } from '@tanstack/react-store'

function ChatPanel() {
  // Auto-subscribes â€” component re-renders when these change
  const messages   = useStore(chatStore, s => s.messages);
  const loading    = useStore(chatStore, s => s.loading);
  const streaming  = useStore(chatStore, s => s.streamingText);
  const toolPhase  = useStore(chatStore, s => s.toolPhase);
  const reasoning  = useStore(chatStore, s => s.reasoningText);
  const inputText  = useStore(chatStore, s => s.inputText);

  return html`
    <div class="chat-panel">
      ${/* Message list */}
      ${messages.map(m => html`
        <div class="msg msg-${m.role}" key=${m.id}>
          ${m.role === "tool"
            ? html`<span class="tool-badge">ğŸ”§ ${m.toolName}</span>`
            : m.content
          }
        </div>
      `)}

      ${/* Thinking bubble */}
      ${reasoning && html`
        <div class="thinking">ğŸ’­ ${reasoning}</div>
      `}

      ${/* Tool execution indicator */}
      ${toolPhase && html`
        <div class="tool-phase">âš™ï¸ ${toolPhase}</div>
      `}

      ${/* Input */}
      <input
        value=${inputText}
        onInput=${(e) => chatStore.setState(s => ({
          ...s, inputText: e.target.value
        }))}
        onKeyDown=${(e) => {
          if (e.key === "Enter" && !loading) {
            sendMessage(inputText);
            chatStore.setState(s => ({ ...s, inputText: "" }));
          }
        }}
        disabled=${loading}
        placeholder=${loading ? "Thinking..." : "Type a message..."}
      />
    </div>
  `;
}
```

### Settings Panel

```typescript
function SettingsPanel() {
  const cfg = useStore(settingsStore, s => s);

  return html`
    <div class="settings">
      <label>Model</label>
      <select
        value=${cfg.llmModel}
        onChange=${(e) => settingsStore.setState(s => ({
          ...s, llmModel: e.target.value
        }))}
      >
        <option value="moonshotai/Kimi-K2.5">Kimi K2.5</option>
        <option value="Qwen/Qwen3-235B-A22B">Qwen3 235B</option>
        <option value="meta-llama/Llama-3.3-70B">Llama 3.3</option>
      </select>

      <label>Temperature: ${cfg.temperature}</label>
      <input type="range" min="0" max="2" step="0.05"
        value=${cfg.temperature}
        onInput=${(e) => settingsStore.setState(s => ({
          ...s, temperature: parseFloat(e.target.value)
        }))}
      />

      <label>Max Tokens</label>
      <input type="number" min="1000" max="30000"
        value=${cfg.maxTokens}
        onChange=${(e) => settingsStore.setState(s => ({
          ...s, maxTokens: parseInt(e.target.value)
        }))}
      />

      <label>Enabled Tools</label>
      ${TOOL_DEFS.map(t => html`
        <label key=${t.function.name}>
          <input type="checkbox"
            checked=${!cfg.disabledTools.includes(t.function.name)}
            onChange=${() => {
              const disabled = cfg.disabledTools.includes(t.function.name)
                ? cfg.disabledTools.filter(n => n !== t.function.name)
                : [...cfg.disabledTools, t.function.name];
              settingsStore.setState(s => ({
                ...s, disabledTools: disabled
              }));
            }}
          />
          ${t.function.name}
        </label>
      `)}
    </div>
  `;
}
```

---

## 15. Adapting for Different Apps

The pattern is identical. Only change `TOOL_DEFS`, `executeTool()`, and
the system prompt.

### IDE / Code Assistant

```typescript
const TOOLS = [
  tool("read_file",    "Read a project file",
       { path: "string" }),
  tool("edit_file",    "Replace text in a file",
       { path: "string", old_str: "string", new_str: "string" }),
  tool("run_command",  "Run a shell command",
       { cmd: "string" }),
  tool("search_code",  "Grep for a pattern",
       { pattern: "string", path: "string" }),
];
```

### E-Commerce Bot

```typescript
const TOOLS = [
  tool("search_products", "Search catalog",
       { query: "string", category: "string?", max_price: "number?" }),
  tool("add_to_cart",     "Add item to cart",
       { product_id: "string", quantity: "number" }),
  tool("get_cart",        "View current cart contents", {}),
  tool("apply_coupon",    "Apply discount code",
       { code: "string" }),
];
```

### Game NPC

```typescript
const TOOLS = [
  tool("move_to",      "Move NPC to world coordinates",
       { x: "number", y: "number", z: "number" }),
  tool("attack",       "Attack target with ability",
       { target_id: "string", ability: "string" }),
  tool("say_dialog",   "Speak with emotion",
       { text: "string", emotion: "string" }),
  tool("check_inventory", "Check player's items", {}),
];
```

### Dashboard / Analytics

```typescript
const TOOLS = [
  tool("run_query",     "Execute SQL against analytics DB",
       { sql: "string" }),
  tool("create_chart",  "Generate a chart",
       { type: "string", data_key: "string", title: "string" }),
  tool("send_alert",    "Send a Slack alert",
       { channel: "string", message: "string" }),
  tool("get_metrics",   "Get current system metrics", {}),
];
```

---

## 16. Gotchas & Debugging

### Common Mistakes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMMON MISTAKES                            â”‚
â”‚                                                              â”‚
â”‚  âŒ Parsing arguments before stream ends                     â”‚
â”‚     â†’ Arguments arrive in fragments! Wait for [DONE].        â”‚
â”‚                                                              â”‚
â”‚  âŒ Missing tool_call_id on tool result messages              â”‚
â”‚     â†’ API returns 400. MUST match the id from tool_calls.    â”‚
â”‚                                                              â”‚
â”‚  âŒ Forgetting the assistant message with tool_calls          â”‚
â”‚     â†’ Conversation must show: assistant(tools) â†’ tool result â”‚
â”‚     â†’ If you skip the assistant msg, the API rejects it.     â”‚
â”‚                                                              â”‚
â”‚  âŒ Sending tool results as role:"user"                       â”‚
â”‚     â†’ Must be role:"tool" with tool_call_id and name.        â”‚
â”‚                                                              â”‚
â”‚  âŒ Not handling the recursive loop                           â”‚
â”‚     â†’ LLM may call more tools after seeing results!          â”‚
â”‚     â†’ Always check toolCalls.length after each stream.       â”‚
â”‚                                                              â”‚
â”‚  âŒ Huge tool results (>10KB)                                â”‚
â”‚     â†’ Wastes context window. Summarize or truncate.          â”‚
â”‚                                                              â”‚
â”‚  âŒ No try/catch in executeTool                               â”‚
â”‚     â†’ If a tool throws, the whole loop breaks.               â”‚
â”‚     â†’ Always return { ok: false, reason: "..." }.            â”‚
â”‚                                                              â”‚
â”‚  âŒ Not clearing reasoning when content starts                â”‚
â”‚     â†’ Kimi sends reasoning_content THEN content.             â”‚
â”‚     â†’ Clear the thinking indicator when text arrives.        â”‚
â”‚                                                              â”‚
â”‚  âŒ TanStack Store subscribe() gotcha                         â”‚
â”‚     â†’ .subscribe() returns { unsubscribe }, NOT a function!  â”‚
â”‚     â†’ âœ… const { unsubscribe } = store.subscribe(cb)         â”‚
â”‚     â†’ âŒ const unsub = store.subscribe(cb); unsub()          â”‚
â”‚                                                              â”‚
â”‚  âŒ Exposing API keys in browser JS                           â”‚
â”‚     â†’ Always proxy through your server.                      â”‚
â”‚     â†’ Never put WANDB_API_KEY in frontend code.              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Debugging SSE Streams

```typescript
// Log raw SSE data to console:
for (const line of lines) {
  if (!line.startsWith("data: ")) continue;
  console.log("[SSE]", line.slice(6, 200));
  // ...
}
```

### Testing Tool Calls Without an LLM

```typescript
// Create a mock SSE stream:
const mockSSE = [
  'data: {"choices":[{"delta":{"tool_calls":[{"index":0,"id":"test_1","function":{"name":"get_weather","arguments":""}}]}}]}',
  'data: {"choices":[{"delta":{"tool_calls":[{"index":0,"function":{"arguments":"{\\"city\\":\\"Tokyo\\"}"}}]}}]}',
  'data: [DONE]',
].join("\n") + "\n";

const blob = new Blob([mockSSE]);
const resp = new Response(blob, {
  headers: { "Content-Type": "text/event-stream" }
});
await streamResponse(resp);
// â†’ executeTool("get_weather", {city:"Tokyo"}) gets called
```

### Verifying Message Order

```typescript
// Before sending, log the conversation:
const msgs = serializeMsgs(chatStore.state.messages);
console.table(msgs.map(m => ({
  role: m.role,
  has_tools: !!m.tool_calls?.length,
  tool_call_id: m.tool_call_id || "-",
  content: (m.content || "").slice(0, 50),
})));
```

---

## 17. Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TOOL CALLING CHEAT SHEET                          â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ TANSTACK STORE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  new Store<T>({ ... })           create store                â”‚
â”‚  store.setState(s => ({...}))    update                      â”‚
â”‚  useStore(store, s => s.field)   read in component           â”‚
â”‚  store.state.field               read outside component      â”‚
â”‚  batch(() => { ... })            batch updates               â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ REQUEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  POST /api/chat                                              â”‚
â”‚  { messages, tools, tool_choice:"auto", stream:true }        â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ SSE PARSING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  delta.reasoning_content  â†’  thinking (Kimi/DeepSeek)        â”‚
â”‚  delta.tool_calls[i]      â†’  accumulate by index             â”‚
â”‚  delta.content             â†’  text response                  â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ AFTER STREAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  if toolCalls.length > 0:                                    â”‚
â”‚    1. Attach tool_calls to assistant message                 â”‚
â”‚    2. Execute each tool â†’ get result                         â”‚
â”‚    3. Add role:"tool" msgs (with tool_call_id!)              â”‚
â”‚    4. sendFollowUp() â†’ recursive streamResponse()            â”‚
â”‚  else:                                                       â”‚
â”‚    Done. Set loading = false.                                â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ MESSAGE ORDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  system â†’ user â†’ assistant(tool_calls) â†’ tool â†’ tool â†’       â”‚
â”‚  assistant(text) â†’ user â†’ ...                                â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ W&B INFERENCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  URL:    api.inference.wandb.ai/v1/chat/completions          â”‚
â”‚  Auth:   Bearer wandb_v1_...                                 â”‚
â”‚  Header: OpenAI-Project: org/project                         â”‚
â”‚  Model:  moonshotai/Kimi-K2.5 (default)                     â”‚
â”‚                                                              â”‚
â”‚  â”€â”€ KEY RULES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  â€¢ Never parse arguments mid-stream                          â”‚
â”‚  â€¢ tool_call_id MUST match tool_calls[].id                   â”‚
â”‚  â€¢ tool content MUST be JSON.stringify'd                      â”‚
â”‚  â€¢ Always try/catch in executeTool                           â”‚
â”‚  â€¢ API keys â†’ server only, never browser                     â”‚
â”‚  â€¢ subscribe() returns { unsubscribe }, not a function       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Listing

For a working implementation, you need:

```
your-app/
â”œâ”€â”€ serve.ts          # Bun server with /api/chat proxy
â”œâ”€â”€ app.ts            # Frontend with TanStack Store + stream parser
â”œâ”€â”€ package.json      # @tanstack/store, @tanstack/preact-store
â””â”€â”€ .env              # WANDB_API_KEY=wandb_v1_...
```

```bash
# Install
bun add @tanstack/store @tanstack/preact-store

# Run
WANDB_API_KEY=wandb_v1_... bun serve.ts
```

---

*Works with any OpenAI-compatible API: W&B Inference, OpenAI, Anthropic,
Together, Fireworks, Groq, Ollama, vLLM, etc. Just change the URL and auth.*
